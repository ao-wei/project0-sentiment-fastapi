from dataclasses import dataclass # ä»æ ‡å‡†åº“ dataclasses æ¨¡å—å¯¼å…¥ dataclass è£…é¥°å™¨
from typing import Tuple

import torch
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, PreTrainedTokenizerBase

@dataclass # å¸®æˆ‘è‡ªåŠ¨ç”Ÿæˆæ„é€ å‡½æ•°ã€æ‰“å°æ–¹æ³•ç­‰çš„â€˜æ•°æ®ç±»â€˜è£…é¥°å™¨ï¼Œç”¨äºå­˜é…ç½®ã€å‚æ•°ã€ç®€å•æ•°æ®ç»“æ„ç­‰
class TrainConfig(): # æ›´ä¼˜é›…çš„é…ç½®ç±»å†™æ³•
    # é»˜è®¤æ¨¡å‹ï¼šdistilroberta-base
    model_name: str = "distilroberta-base"

    # é»˜è®¤æ•°æ®é›†ï¼šimdb
    dataset_name: str = "imdb"

    max_length: int = 256

    seed: int = 666

def get_device() -> torch.device:
    """è·å–è®­ç»ƒè®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨ Apple GPU(mps), å…¶æ¬¡ CUDAï¼Œæœ€å CPU"""
    if torch.backends.mps.is_available():
        print("MPS is available, using Apple GPU (MPS).")
        return torch.device("mps")
    elif torch.cuda.is_available():
        print("CUDA is available, using Nvidia GPU.")
        return torch.device("cuda")
    else:
        print("no GPU found, using CPU")
        return torch.device("cpu")
    
# Tokenizeræ‰€åšçš„å·¥ä½œï¼šæŒ‰ç…§ç›¸å¯¹åº”çš„æ¨¡å‹
# 1ã€è§„èŒƒæ–‡æœ¬ï¼ˆå¦‚è½¬å°å†™ï¼Œå»æ‰å¥‡æ€ªå­—ç¬¦ç­‰ï¼‰
# 2ã€å°†ä¸€å¥è¯åˆ‡æˆtokenæˆ–å­è¯
# 3ã€å°†æ¯ä¸ªtokenæ˜ å°„æˆæ•´æ•°ID
# 4ã€å¤„ç†é•¿åº¦ & æ©ç 
# 5ã€æ•´ç†æˆæ¨¡å‹è¦çš„å­—æ®µæ ¼å¼
# tokenizer = å›ºå®šçš„ç®—æ³•æ¡†æ¶ + â€œåœ¨å¤§è¯­æ–™ä¸Šè®­ç»ƒå‡ºæ¥çš„è¯è¡¨/è§„åˆ™â€œ

# tokenizeræŠŠæ–‡æœ¬å˜æˆä¸€ä¸²æ•´æ•° ID
# Embeddingå±‚å°±æ˜¯å°†è¿™äº›æ•´æ•°IDå˜æˆâ€œæœ‰æ„ä¹‰çš„å‘é‡â€œï¼Œä¾›Transformerä½¿ç”¨
def load_data_and_tokenizer(
    config: TrainConfig
) -> Tuple[DatasetDict, DatasetDict, PreTrainedTokenizerBase]:
    torch.manual_seed(config.seed)

    print(f"Loading dataset: {config.dataset_name}")
    raw_datasets = load_dataset(config.dataset_name)
    print(raw_datasets)

    example = raw_datasets["train"][0]
    print("\nRaw example from train dataset:")
    print(example)

    print(f"\n Loading tokenizer: {config.model_name}")
    tokenizer = AutoTokenizer.from_pretrained(config.model_name, use_fast=True)

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length = config.max_length,
        )
    
    print("\n Tokenizing dataset...")
    # å°† raw_datasets ä¸­çš„æ ·æœ¬ï¼ŒæŒ‰æ‰¹æ¬¡è¯»å‡ºæ¥ï¼Œæ¯ä¸€æ‰¹äº¤ç»™ tokenize_function
    # tokenize_function è´Ÿè´£ç”¨å¯¹åº”æ¨¡å‹çš„ tokenizerï¼ŒæŠŠè¿™ä¸€æ‰¹é‡Œçš„ text åˆ—è½¬ä¸º input_ids å’Œ attention_mask
    # å¹¶ä¸åŸæ‰¹æ¬¡åˆå¹¶ï¼Œå†ä¸¢æ‰textåˆ—
    # map æ˜¯ä¸€ç§å‡½æ•°å¼é£æ ¼çš„æ“ä½œï¼Œä¸ç›´æ¥ä¿®æ”¹åŸæ¥çš„ Dataï¼Œè€Œæ˜¯åŸºäºåŸæ•°æ®æ„å»ºæ–°çš„æ•°æ®é›†
    tokenized_datasets = raw_datasets.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"], # tokenizedå¤„ç†è¿‡åï¼Œtextå·²æ²¡æœ‰ä½œç”¨
        desc="Tokenizing", # ç»™è¿›åº¦æ¡çœ‹çš„è¯´æ˜æ–‡å­—
    )

    if "label" in tokenized_datasets["train"].column_names:
        tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

    tokenized_datasets.set_format( # å°±åœ°ä¿®æ”¹è§†å›¾è®¾ç½®
        type="torch", # è‡ªåŠ¨å°†å®ƒä»¬ä»¥torch.Tensorå½¢å¼è¿”å›ï¼ˆåº•å±‚æ•°æ®æ²¡å˜ï¼Œè¿”å›å½¢å¼å˜äº†ï¼‰
        columns=["input_ids", "attention_mask", "labels"], # ä»¥åä» tokenized_datasets å–æ•°æ®æ—¶ï¼Œåªç»™è¿™ä¸‰åˆ—
    )

    print("\n Tokenization done. Tokenized dataset:")
    print(tokenized_datasets)

    print("\n One tokenized example from train dataset:")
    print(tokenized_datasets["train"][0])

    return raw_datasets, tokenized_datasets, tokenizer

def main():
    config = TrainConfig()

    print("==== TrainConfig ====")
    print(config)

    device = get_device()
    print("Using device: ", device)

    raw_datasets, tokenized_datasets, tokenizer = load_data_and_tokenizer(config)

    print("\nğŸ‰ Pipeline check finished.")
    print("You now have:")
    print("  - raw_datasets      (with text + label)")
    print("  - tokenized_datasets(input_ids + attention_mask + labels)")
    print("  - tokenizer         (distilroberta-base)")
    print("\nNext step: we will add model + TrainingArguments + Trainer on top of this.\n")

# ç›´æ¥æ‰§è¡Œæ—¶ï¼Œå†…ç½®å˜é‡__name__ä¸º__main__
# é€šè¿‡å¯¼å…¥æ‰§è¡Œæ—¶ï¼Œå†…ç½®å˜é‡__name__ä¸ºæ¨¡å—å trainï¼ˆæ— ä¸‹åˆ’çº¿ï¼‰
if __name__ == "__main__":
    main()   